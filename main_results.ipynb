{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# Mostrar DataFrames sin saltos de línea\n",
    "pd.set_option('display.expand_frame_repr', False)\n",
    "import ast\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>experiment_id</th>\n",
       "      <th>trial_index</th>\n",
       "      <th>dino_model</th>\n",
       "      <th>normalization</th>\n",
       "      <th>scaler</th>\n",
       "      <th>dim_red</th>\n",
       "      <th>reduction_parameters</th>\n",
       "      <th>clustering</th>\n",
       "      <th>n_clusters</th>\n",
       "      <th>...</th>\n",
       "      <th>penalty_range</th>\n",
       "      <th>noise_not_noise</th>\n",
       "      <th>classification_lvl</th>\n",
       "      <th>lvlm</th>\n",
       "      <th>prompt</th>\n",
       "      <th>eval_method</th>\n",
       "      <th>best_score</th>\n",
       "      <th>homogeneity_global</th>\n",
       "      <th>entropy_global</th>\n",
       "      <th>quality_metric</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>355</td>\n",
       "      <td>small</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>umap</td>\n",
       "      <td>{'metric': 'cosine', 'n_components': 2, 'n_nei...</td>\n",
       "      <td>hdbscan</td>\n",
       "      <td>156</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{-1: 1923, 1: 3143}</td>\n",
       "      <td>3</td>\n",
       "      <td>llava1-6_7b</td>\n",
       "      <td>1</td>\n",
       "      <td>silhouette</td>\n",
       "      <td>0.725811</td>\n",
       "      <td>0.803054</td>\n",
       "      <td>0.657708</td>\n",
       "      <td>1.220988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>361</td>\n",
       "      <td>small</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>umap</td>\n",
       "      <td>{'metric': 'cosine', 'n_components': 2, 'n_nei...</td>\n",
       "      <td>hdbscan</td>\n",
       "      <td>155</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{-1: 2084, 1: 2982}</td>\n",
       "      <td>3</td>\n",
       "      <td>llava1-6_7b</td>\n",
       "      <td>1</td>\n",
       "      <td>silhouette</td>\n",
       "      <td>0.724763</td>\n",
       "      <td>0.803054</td>\n",
       "      <td>0.657708</td>\n",
       "      <td>1.220988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>small</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>umap</td>\n",
       "      <td>{'metric': 'euclidean', 'n_components': 2, 'n_...</td>\n",
       "      <td>hdbscan</td>\n",
       "      <td>162</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{-1: 1757, 1: 3309}</td>\n",
       "      <td>3</td>\n",
       "      <td>llava1-6_7b</td>\n",
       "      <td>1</td>\n",
       "      <td>silhouette</td>\n",
       "      <td>0.721921</td>\n",
       "      <td>0.803054</td>\n",
       "      <td>0.657708</td>\n",
       "      <td>1.220988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>small</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>umap</td>\n",
       "      <td>{'metric': 'euclidean', 'n_components': 2, 'n_...</td>\n",
       "      <td>hdbscan</td>\n",
       "      <td>233</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{-1: 1769, 1: 3297}</td>\n",
       "      <td>3</td>\n",
       "      <td>llava1-6_7b</td>\n",
       "      <td>1</td>\n",
       "      <td>silhouette</td>\n",
       "      <td>0.719861</td>\n",
       "      <td>0.803054</td>\n",
       "      <td>0.657708</td>\n",
       "      <td>1.220988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>small</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>umap</td>\n",
       "      <td>{'metric': 'euclidean', 'n_components': 2, 'n_...</td>\n",
       "      <td>hdbscan</td>\n",
       "      <td>130</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{-1: 1895, 1: 3171}</td>\n",
       "      <td>3</td>\n",
       "      <td>llava1-6_7b</td>\n",
       "      <td>1</td>\n",
       "      <td>silhouette</td>\n",
       "      <td>0.717324</td>\n",
       "      <td>0.803054</td>\n",
       "      <td>0.657708</td>\n",
       "      <td>1.220988</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  experiment_id  trial_index dino_model  normalization scaler dim_red                               reduction_parameters clustering  n_clusters  ... penalty_range      noise_not_noise classification_lvl         lvlm  prompt eval_method  best_score homogeneity_global  entropy_global  quality_metric\n",
       "0           0              1          355      small           True    NaN    umap  {'metric': 'cosine', 'n_components': 2, 'n_nei...    hdbscan         156  ...           NaN  {-1: 1923, 1: 3143}                  3  llava1-6_7b       1  silhouette    0.725811           0.803054        0.657708        1.220988\n",
       "1           1              1          361      small           True    NaN    umap  {'metric': 'cosine', 'n_components': 2, 'n_nei...    hdbscan         155  ...           NaN  {-1: 2084, 1: 2982}                  3  llava1-6_7b       1  silhouette    0.724763           0.803054        0.657708        1.220988\n",
       "2           2              1           13      small           True    NaN    umap  {'metric': 'euclidean', 'n_components': 2, 'n_...    hdbscan         162  ...           NaN  {-1: 1757, 1: 3309}                  3  llava1-6_7b       1  silhouette    0.721921           0.803054        0.657708        1.220988\n",
       "3           3              1            7      small           True    NaN    umap  {'metric': 'euclidean', 'n_components': 2, 'n_...    hdbscan         233  ...           NaN  {-1: 1769, 1: 3297}                  3  llava1-6_7b       1  silhouette    0.719861           0.803054        0.657708        1.220988\n",
       "4           4              1           19      small           True    NaN    umap  {'metric': 'euclidean', 'n_components': 2, 'n_...    hdbscan         130  ...           NaN  {-1: 1895, 1: 3171}                  3  llava1-6_7b       1  silhouette    0.717324           0.803054        0.657708        1.220988\n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading csv with results\n",
    "df = pd.read_csv(\"results_top_trials_v2.csv\", sep=\";\", na_values=\"NaN\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "\n",
    "def filter_results(df_res):\n",
    "    # Crear una copia del DataFrame\n",
    "    df_res = df_res.copy()\n",
    "\n",
    "    # Manejar valores NaN antes de aplicar la conversión\n",
    "    df_res['reduction_parameters'] = df_res['reduction_parameters'].fillna('{\"n_components\":\"all\"}')\n",
    "    df_res['scaler'] = df_res['scaler'].fillna('None')\n",
    "\n",
    "    # Extraer 'n_components'\n",
    "    df_res['n_components'] = df_res['reduction_parameters'].apply(\n",
    "        lambda x: ast.literal_eval(x).get('n_components', 'all')\n",
    "    )\n",
    "\n",
    "    # Calcular y agregar 'noise_ratio'\n",
    "    df_res['noise_ratio'] = df_res['noise_not_noise'].apply(\n",
    "        lambda x: round(ast.literal_eval(x).get(-1, 0) / 5066, 3)\n",
    "    )\n",
    "\n",
    "    df_res['best_score'] = round(df_res['best_score'],3)\n",
    "\n",
    "    # Eliminar la columna de parámetros reducidos\n",
    "    df_res = df_res.drop([\"reduction_parameters\"], axis=1)\n",
    "\n",
    "    # Seleccionar los 10 mejores ensayos únicos de cada algoritmo\n",
    "    top10_unique_trials = df_res.sort_values(\n",
    "        by=\"best_score\", ascending=False\n",
    "    ).groupby('clustering').head(10)\n",
    "\n",
    "    # Ordenar los resultados\n",
    "    ascending =  False if df_res['eval_method'].unique() in (\"silhouette\",\"silhouette_noise\") else True\n",
    "    top10_sorted = top10_unique_trials.sort_values(by=\"best_score\", ascending=ascending)\n",
    "\n",
    "    # Devolver el DataFrame actualizado\n",
    "    return top10_sorted[[\n",
    "        \"experiment_id\", \"trial_index\", \"clustering\",\"normalization\", \"scaler\",\n",
    "        \"dim_red\", \"n_components\", \"noise_ratio\", \"n_clusters\", \"eval_method\", \"best_score\"\n",
    "    ]]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "\n",
    "def filter_results_less_noise_ratio(df_res):\n",
    "    # Crear una copia del DataFrame\n",
    "    df_res = df_res.copy()\n",
    "\n",
    "    # Manejar valores NaN antes de aplicar la conversión\n",
    "    df_res['reduction_parameters'] = df_res['reduction_parameters'].fillna('{\"n_components\":\"all\"}')\n",
    "    df_res['scaler'] = df_res['scaler'].fillna('None')\n",
    "\n",
    "    # Extraer 'n_components'\n",
    "    df_res['n_components'] = df_res['reduction_parameters'].apply(\n",
    "        lambda x: ast.literal_eval(x).get('n_components', 'all')\n",
    "    )\n",
    "\n",
    "    # Calcular y agregar 'noise_ratio'\n",
    "    df_res['noise_ratio'] = df_res['noise_not_noise'].apply(\n",
    "        lambda x: round(ast.literal_eval(x).get(-1, 0) / 5066, 3)\n",
    "    )\n",
    "\n",
    "    df_res['best_score'] = round(df_res['best_score'], 3)\n",
    "\n",
    "    # Eliminar la columna de parámetros reducidos\n",
    "    df_res = df_res.drop([\"reduction_parameters\"], axis=1)\n",
    "\n",
    "    # Seleccionar los 10 mejores ensayos únicos de cada algoritmo\n",
    "    top10_unique_trials = df_res.sort_values(\n",
    "        by=[\"noise_ratio\", \"best_score\"], \n",
    "        ascending=[True, False]  # Menor noise_ratio, mejor best_score\n",
    "    ).groupby('clustering').head(10)\n",
    "\n",
    "    # Devolver el DataFrame actualizado\n",
    "    return top10_unique_trials[[\n",
    "        \"experiment_id\", \"trial_index\", \"clustering\", \"normalization\", \"scaler\", \n",
    "        \"dim_red\", \"n_components\", \"noise_ratio\", \"n_clusters\", \"eval_method\", \"best_score\"\n",
    "    ]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     experiment_id  trial_index clustering  normalization    scaler dim_red  n_components  noise_ratio  n_clusters     eval_method  best_score\n",
      "819             29          140    hdbscan           True      None    umap             5        0.000          46  davies_bouldin       0.465\n",
      "577             21           80    hdbscan           True  standard    umap             2        0.000          48  davies_bouldin       0.421\n",
      "487             18           80    hdbscan           True      None    umap             2        0.000          54  davies_bouldin       0.406\n",
      "817             29           80    hdbscan           True      None    umap             2        0.000          54  davies_bouldin       0.406\n",
      "816             29            0    hdbscan           True      None    umap             2        0.000          51  davies_bouldin       0.382\n",
      "576             21            0    hdbscan           True  standard    umap             2        0.000          67  davies_bouldin       0.357\n",
      "578             21           20    hdbscan           True  standard    umap             3        0.001          40  davies_bouldin       0.436\n",
      "486             18            0    hdbscan           True      None    umap             2        0.001          55  davies_bouldin       0.377\n",
      "579             21           40    hdbscan           True  standard    umap             4        0.060         229  davies_bouldin       0.451\n",
      "489             18          120    hdbscan           True      None    umap             4        0.068         234  davies_bouldin       0.410\n"
     ]
    }
   ],
   "source": [
    "df_res = df[(df[\"eval_method\"] == \"davies_bouldin\") & \n",
    "                        (df[\"n_clusters\"] > 2) &\n",
    "                        (df[\"clustering\"] == \"hdbscan\") &\n",
    "                          (df[\"dino_model\"] == \"base\")][[\"experiment_id\", \"trial_index\", \"clustering\",\"normalization\", \"scaler\",\"dim_red\", \n",
    "                                                         \"reduction_parameters\",\"noise_not_noise\", \"n_clusters\", \"eval_method\", \"best_score\"]].drop_duplicates()\n",
    "\n",
    "#print(filter_results(df_res))\n",
    "print(filter_results_less_noise_ratio(df_res))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "\n",
    "def filter_results_best_homegeneity_global(df_res, with_noise=False):\n",
    "    # Crear una copia del DataFrame\n",
    "    df_res = df_res.copy()\n",
    "\n",
    "    # Manejar valores NaN antes de aplicar la conversión\n",
    "    df_res['reduction_parameters'] = df_res['reduction_parameters'].fillna('{\"n_components\":\"all\"}')\n",
    "    df_res['scaler'] = df_res['scaler'].fillna('None')\n",
    "\n",
    "    # Extraer 'n_components'\n",
    "    df_res['n_components'] = df_res['reduction_parameters'].apply(\n",
    "        lambda x: ast.literal_eval(x).get('n_components', 'all')\n",
    "    )\n",
    "\n",
    "    # Calcular y agregar 'noise_ratio'\n",
    "    df_res['noise_ratio'] = df_res['noise_not_noise'].apply(\n",
    "        lambda x: round(ast.literal_eval(x).get(-1, 0) / 5066, 3)\n",
    "    )\n",
    "\n",
    "    # Redondear valores relevantes\n",
    "    df_res['best_score'] = round(df_res['best_score'], 3)\n",
    "    df_res['homogeneity_global'] = round(df_res['homogeneity_global'], 3)\n",
    "    df_res['quality_metric'] = round(df_res['quality_metric'], 3)\n",
    "\n",
    "    df_res['quality_metric_noise_ratio'] = df_res['quality_metric']/df_res['noise_ratio']\n",
    "\n",
    "    # Eliminar la columna de parámetros reducidos\n",
    "    df_res = df_res.drop([\"reduction_parameters\"], axis=1)\n",
    "\n",
    "    # Ordenar el DataFrame por quality_metric con menor noise_ratio\n",
    "    df_sorted = df_res.sort_values(\n",
    "        by=[\"noise_ratio\", \"quality_metric\"], \n",
    "        ascending=[True, False]\n",
    "    )\n",
    "\n",
    "    # Obtener los 5 mejores experimentos\n",
    "    top5_experiments = df_sorted.groupby('experiment_id').head(1)['experiment_id'].unique()\n",
    "    print(top5_experiments)\n",
    "    # Filtrar los mejores experimentos\n",
    "    top_experiment_trials = df_sorted[df_sorted['experiment_id'].isin(top5_experiments)]\n",
    "\n",
    "    # Obtener los 3 mejores ensayos de cada experimento\n",
    "    top3_trials_per_experiment = top_experiment_trials.groupby('experiment_id').head(3)\n",
    "\n",
    "\n",
    "\n",
    "    # Devolver el DataFrame actualizado\n",
    "    return top3_trials_per_experiment[[\n",
    "        \"experiment_id\", \"best_trial_index\", \"clustering\", \"dino_model\", \n",
    "        \"dim_red\", \"n_components\", \"noise_ratio\", \"n_clusters\", \n",
    "        \"eval_method\", \"best_score\", \"lvlm\", \"prompt\", \"quality_metric\"\n",
    "    ]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19 28 22 66 65 47 46 43 42 45 44 64  5 12 41  1 11 40  9  6 13  8 16  3\n",
      "  4 62]\n",
      "     experiment_id  best_trial_index clustering dino_model dim_red  n_components  noise_ratio  n_clusters       eval_method  best_score          lvlm  prompt  quality_metric\n",
      "163             28                60    hdbscan       base    umap             5        0.091         262        silhouette       0.640   llava1-6_7b       2           1.494\n",
      "391             66                45    hdbscan       base    umap            11        0.146         171      davies_noise       0.548   llava1-6_7b       2           1.432\n",
      "127             22                60    hdbscan       base    umap             5        0.101         214        silhouette       0.642   llava1-6_7b       2           1.391\n",
      "109             19                32    hdbscan       base    umap             4        0.089         213  silhouette_noise       0.620   llava1-6_7b       2           1.376\n",
      "162             28                60    hdbscan       base    umap             5        0.091         262        silhouette       0.640   llava1-6_7b       1           1.144\n",
      "165             28                60    hdbscan       base    umap             5        0.091         262        silhouette       0.640  llava1-6_13b       2           1.135\n",
      "126             22                60    hdbscan       base    umap             5        0.101         214        silhouette       0.642   llava1-6_7b       1           1.081\n",
      "129             22                60    hdbscan       base    umap             5        0.101         214        silhouette       0.642  llava1-6_13b       2           1.072\n",
      "108             19                32    hdbscan       base    umap             4        0.089         213  silhouette_noise       0.620   llava1-6_7b       1           1.052\n",
      "111             19                32    hdbscan       base    umap             4        0.089         213  silhouette_noise       0.620  llava1-6_13b       2           1.043\n"
     ]
    }
   ],
   "source": [
    "## TOP 5 KMEANS SILHOUETTE SMALL\n",
    "\n",
    "# Loading csv with results\n",
    "df = pd.read_csv(\"results.csv\", sep=\";\", na_values=\"NaN\")\n",
    "df.head()\n",
    "\n",
    "with_noise = False\n",
    "df_res = df[(df[\"n_clusters\"] > 2) &\n",
    "            (df[\"lvlm\"] != \"llava1-5_7b\") &\n",
    "            (df[\"dim_red\"].notna()) &\n",
    "             (df[\"clustering\"] == \"hdbscan\") &\n",
    "             (df[\"n_clusters\"] > 100) ][\n",
    "                              [\"experiment_id\", \"best_trial_index\", \"clustering\",\"normalization\", \"scaler\",\"dim_red\", \n",
    "                              \"reduction_parameters\",\"noise_not_noise\", \"n_clusters\", \"eval_method\", \"dino_model\",\n",
    "                              \"lvlm\",\"prompt\",\"homogeneity_global\",\"quality_metric\",\"best_score\"]].drop_duplicates()\n",
    "\n",
    "#print(filter_results(df_res))\n",
    "#print(filter_results_less_noise_ratio(df_res))\n",
    "with pd.option_context('display.max_rows', None):\n",
    "    df_def = filter_results_best_homegeneity_global(df_res, False).head(10)\n",
    "    df_def = df_def[df_def[\"score\"]].sort_values(by=\"quality_metric\", ascending=False, inplace=True)\n",
    "    print(df_def)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
